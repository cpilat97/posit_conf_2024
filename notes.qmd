---
title: "CP - Posit Conf Notes, 2024"
format: 
  html: 
    toc: true
    toc-location: "left"
    page-layout: "full"
---

## 2024-08-13 

### Keynote - Updates from Posit @ 0900hrs

  - Updated versions of Quarto support `typst` for better, faster rendering of PDF docs. 
      - Fast, easy, customizable
      - A key power here is that html tables are easily converted under-the-hood and won't really break anything. 
  - Native `julia` engine now as well
  - TONS of quarto extensions 
    - Find Extensions - m.canouil.dev/quarto-extensions/
  - R for WebAssembly 
    - webR - executing R code in the web browser
    - webr.r-wasm.org
    - `WebAssembly`
      - Portable binary code format
      - high-perf webapps
      -most mod browsers
    - Shinylive 
    - `Quarto Live` - interactive docs & code exercises in Quarto powered by wasm
      - using a `webr` codeblock if you have the quarto webr extensions download
  - Snowflake & Databricks integrations
    - Infrastructure Management
    - Data Access
    - Application Deployment

### Positron Track - 1020 - 1140hrs

#### Exploratory Data Science with Python & Positron - Isabel Zimmerman

  - The data science life cycle doesn't really change even if the tools we use change
  - How does Positron support DS? 
    - Similar to RStudio - you have a sandbox w/ console, text editor, variable panes, and plot viewers
  - Interpreter selector that allows you to quickly select between python & r versions
    - Will remember per projects what versions of stuff you're using
  - DS is very experimental, iterative work: so you have an interactive, built-in console (much like R)
  - The connections pane give s you more information regarding what tables are in your db. 
  - Positron has a built in help pane much like rstudio, allowing you to grab the information directly in the IDE. 


#### Data Explorer in Positron - Tom Mock 

  - Summary Panels associated with the data viewer
    - Col Types, persistent filter bar, & summary panel
  - Data Viewer Grid
  - Grid design: R (dfs, tibble, data.table) & python (polars, pandas)
  - The code you write will automatically update the viewer (no more refreshing like in rstudio). 
  - Highly scalable: 
    - Handles millions of rows & Millions of columns
  - Sort multiple columns at a time and it will number them so you can keep track of whats going on. 
  - Auto column width
  - Filter bar sits outside the data with a designated design so it keeps you aware of things going on. 
  - Summary Panel design 
    - Type & name 
    - Missing data views so you don't need to do operations to find these
  - Upcoming: 
    - Sparkline histograms
    - Remote database conns

#### How Positron Works - Jonathan McPHerson

  - Why no build on RStudio? 
    - It was difficult to iterate on RStudio when trying to build out a polyglot IDE
    - RStudio is a 2 process sys. 
      - 1 process runs R, saves files, ui, etc... (which is why it gets slow when things are computing)
      - 1 process is electron - the browser
    - Python needs to run inside of R 
    - Theres also a choice between data featurues or IDE features
  - Positron is a fork of Code OSS
    - Which is the foundation of VSCode
    - Cursor AI Editor
    - and now Positron
    - what does this ultimately mean? You can sep the IDE and add in many things to it
  - Why not just build an extension to VSCode? 
    - Extensions are separate from the IDE, they're their own sandbox. So extensions are rather limited
    - Ephemeral & restarted any time
    - Stop running 
    - Doesn't make sense as a foundation for other extenstions
  - How does R & Py talk w/ these extensinos
    - R talks to the R Extension which talks to the VS Code API ( code completion & debugs) &&&& the positron ide for viewers, 
    - R & Python are treated as extensions...why is this important? When something Crashes, the ide just plugs along there's no restarting the entire ide
    - Allows Positron to extend to other langs in the future (eg `Julia`)
    - Theres a public API that anyone can write to and that can be brought to the positron core
  - Positron uses standard protocols for standard funcs
    - meaning they use protocols that are standard throughout the DS community

#### Introducing Ark, Modern tooling for R - Lionel Henry & Davis Vaughan

  - Positron is multi-lingual 
  - Scope now includes Python
  - Where is R left? 
  - Ark
    - Ark is behind all the auto completions that happen
    - When you're looking for outputs, it goes through Ark
    - Ark is: 
      - Execution: 
      - Assistance: Completions, etc...
      - Debugger 
  - Ark Execution: 
    - Think of code executions as a question. You hit `cmd + enter` asking the code question. 
    - Ark "answers" the question, displaying the results in the console
    - How does this work? 
      - Ark is a jupyter kernal
      - executes the jupyter protocol
  - Code assistance when writing R code
    - Code assistance could look like function signature
    - Code completion
    - Jumping to the function definition
    - Unique to Positron: 
      - Help on hover: in-line documentation
    - These features are similar to the code execution: asking questions & sending back answers. 
    - Built on the language server protocol. 
      - Eg send a "hover-request" send back a "hover response"
  - The "smarts" of Ark is from `tree-sitter` 
    - This is structured representaitons of code
    - The R grammer for tree-sitter needed to be built....which is what Posit did!
    - Github uses tree-sitter. 
      - Posit worked w/ Github to incorportate the R grammer tree-sitter protocol and it works across github now
  - The "debugger"
    - Debuggers allow you to step through code that's throwing an error. 
    - The debugger was implemented via a standardized set of questions and answers
    - The debugger uses the debug adapter protocol
    - Positron will bring you to the cpp code of dplyr
  

  **IMPORTANT** So it seems like one of the biggest advantages here with Positron is that it's implementing a lot of standard protocols that many other languages may have which means we can now take advantage of these protocols ALONG with R and benefit everywhere. 

#### Positron for R and RSudio Users - Jenny Bryan

  - Tips for testdriving Positron
    - Use **workspaces** which is a core concept, and tehse are just folders (analagous to `.Rproject`)
      - Once you're in a folder, everything kinda comes to life
    - Easy to use multiple versions of R and switching between them with fluidity. 
      - Console will always tell you what you're using
      - `rig` package
        - allows you manage multiple R installs (particularlly for macOS users). 
        - r-lib/rig
  - New defaults!
    - Blank slate lifestyle (no saved workspaces, etc...)
  - Command Palette is a core concept in Positron
  - Things that are available in panes in RStudio might be brought into the command palette of positron. 
  - Customizing your layout
  - Exentsions from Open VSX
    - indent-rainbow
    - project manager
  - The little flask icon is for your tests & `testthat`
  - 

The rsconnect publisher is an extension



### Strengthening the R Ecosystem

#### 20+ Years of Reading Data into R - Colin Gillespie

Larger files --> Data --> File Formats --> Engine --> Results

  - So now we have nanoparquet: https://nanoparquet.r-lib.org/
  - Parquet vs RDS: (example compare)
    - RDS uses gzip
      - 115mb
      - write speed: 27sec 
      - read speed 5.7sec
    - parquet uses snappy 
      - 143mb
      - Write speed: 4sec
      - read speed: .3sec
    - parquet also uses gzip (depending)
      - 105mb
      - write speed: 12sec
      - read speed .4 sec
  - `duckdb`
    - using dplyr + duckplyr
    - results
  - Why load data into duckdb? 
    - Its faster 
    - Repeated queries
    - Joins
  - lots of blog posts on jumpingrivers.com

#### Contributing to the R project

  - contributor.r-project.org


### Innovating with Shiny - 1440hrs

#### Wait, thatâ€™s Shiny? Building feature-full, user-friendly interactive data explorers with Shiny and friends - Kiegan Rice

  - Can shiny do that? 
    - Give a home page, somewwhere to land. ho they can explore, etc....
    - Add about & methodology pages 
    - These are some of the basics you can start with. 
  - Taking things a step further
    - Adding in sub-urls via routing
    - Change things reactively based on users selection. 
    - Adding context for what a user is seeing
    - Using shiny bookmarking to save the state of an app
      - Be unconventional about it if you need to be
    - Think about what users want to learn and whwat they might have questions about. 
    - Flip the way you think about the questions and what you want the app to do 
  - Why use shiny to do something? 
  - Why should i do that in shiny? 
    - Draw them in
    - Understand where data comes rom 
    - Show specific views
    - Have ways to explore their own data
  - What do you need? 
    - Understanding the data structure
    - Someone who can build the interface 
    - Connections between the data and the interface
    - Luckily, shiny devs can do all this 
  - sample rep: github.com/kiegan/wait-thats-shiny

#### We CAN have nice Shiny apps: What's new in Shiny's UI & UX - Greg Swinehart

  - Yes, yes we can 
  - `bslib` package
    - Lots of many small quality of life improvements from shiny past to today
  - Shiny Components Gallery (component cheat sheet)
  - Shiny Layouts
  - Shiny Templates (really only shiny for python)

#### Making an App a System - Andrew Bates

  - Best practices for app reusability

#### Empowering Decisions: Advanced Portfolio Analysis and Management through Shiny - Lovekumar Patel

## 2024-08-14

### Keynote - A Future of Data Science - Allen Downey - 0900hrs

  - Data science is a set of tools and processes for ansering questions, resovling debates, and setting standards
  - The gartner hype cycle
    - tech trigger
    - peak of expectations
    - disillousenment
    - slope of enlightnment
    - plateu of progress
  - DS exists b/ statistics missed the boat on computers (lol)
  - Stats try to say all the problems have been solved
  - BUT DS says none of the problems have been solved and we need a set of virtual tools to solve these
  - Variability Hypothesis
  - slop of enlightment: (where we can go from the trough of disillusionment)
    - Data Journalism is sneakily improving data literacy in general
    - complimenting the data capability of governments
    - Improving data literacy 
    - More access to data in general
  - Factfulness - Hans Rosling
  - On long-term trends, almost everything is getting better, and we still have challenging problems, AND our history suggests we can solve them. 
    - Not the end of the world - Hannah Ritchie
  - Negativity bias is a serious threat to our well being b/c it undermines our ability to address the things that actually need to be addressed
  - Use data to understand the world better so we know how to make the world better 
  - Some resources: 
    - Think Stats (oreilly book) - greenteapress.com
    - Elements of Data Science - Allen Doney
    - BRFSS (cdc)
    - Probably overthinking it - ALlen Downey
    - tinyurl.com/dow24pos

### Session 1 - Data Engineering - 1020-1140hrs

#### Reclaiming your weekend with Data Contracts - Nick Pelikan

  - Trust in your outputs is **critical** to be a successful ds team...but how much control do you have over your data *inputs*? 
  - `data mesh` - decentralizing the data architecture 
  - which leads to three problems: 
    - Reliability: will the data be there? 
    - Consistency: data changing? 
    - Autonomy: Can i build things myself? 
  - How do we solve this? Data Contracts 
    - Agreement b/w data producers and data consumers...this means it does not have to include technology. Its ultimately an agreement 
  - What makes a good data contract? 
    - People: 
      1. Contract owner
      2. Responsible
      3. Consulted
    - Expectations: 
      1. Schema
      2. Valid Values
      3. SLAs
      4. Interface
      5. Compliance
      6. Interop
  - So who builds the contracts? Producers? Consumers? 
    - both...its a collab effort not one or the other
  - Tech is helpful but not everything
  -Existing framewlrks: 
    - Many advantages - integrations, easy to standup
    - but have disadvs: single lang, new api, dont encode ownership
  - Literate contracts 
    - Multiglingual, ownership is defined, integrate with existing frameworks
    - but....it adds aditional steps, and since it's more verbose its more effort
  - Theres no right answer, pick a tech that works best for you
  - Adopting data contracts in your org 
    - Expect resistances
    - Articulate value - whats the business val? 
    - Keep it simple - save time & build credibility

#### Demystifying Data Modeling - Kshitij Aranke (dbt labs)

  - Data modeling for data engineers
  - what is data modeling
  - Why it's important
  - How to get started with it
  - Starting with data extracting, then transforming and prep, then assemble your df
    - Visualizing then building
  - The idea is taking these vague big prompts that we might get from a stakeholder or leadership, etc..., teasing it apart, and working through the specific asks. 
    - It ultimately becomes the analytics workflow: 
      - Raw data 
      - Transform 
      - Data [cleaned]
      - output --> ml, bi, op analytics
  - Can we doc & test the workflows though? 
    - your docs might include the general schema of what you're doing, who owns that dataset, the tests important for the tables, etc... 
    - documenting terms and what they mean. 
    - By documenting, `tests` should flow out of the documentation 
  - Data quality pattern
    - write
      - when you're writing new data, you're writing it to a staging platform that goes through tests before sending it to production becuase it's then high-quality
    - audit
    - publish
  - standardization is an important part of a data engineers jobs
  - a data model is a structured rep that orgs and standardizes data to enable & guide human and machine behavior, inform decision-making, and facilitate actions. 
  - data modeling helps you scale
  - how do you get started? 
    - its a process: start small, keep iterating, and have fun
  - data modeling toolkit
  - data modeling is a tool-agnostic process
  - fundamentals of data engineering (book rec)

#### {mirai} and {crew}: next-generation async to supercharge {promises}, Plumber, Shiny, and {targets} - Charlie Gao and Will Landau

  - what is next gen async? 
    - what do we mean?: essentially not waiting around while multiple things happen at once
  - Hows this currently working in `R`? 
    - Well, async is not a first-class async
    - `parallel` is parallel, not async, meaning you still have to wait around for results to return before moving on in your session. 
    - {future} relise on {parallelly} and blocks tasks if theres more going on then the number of workers available
  - Brining first class async to R
    - `nanonext` brings NNG to R
  - `mirai`
    - minimalist async eval framework for R
    - Simply japanese for *future*
    - mirai uses nanonext to deliver first-class async
    - connect thousands of parallel processes and tasks all at once
    - responsive (in the microsecond range)
  - Currently, we use promises 
    - a promising object 
    - `future` which blocks the session
    - future_promise() has never exited experimental
    - Requires constant polling for resolution of each promise
  - Mirai is a promising object
    - native shiny/plumber driven
    - event driven (no polling)
  - `crew` package is an extension built upon `mirai`
    - mirai, crew, cre.cluster, crew.aws.batch
    - crew provides auto-scaling
    - plug-in access
    - mirai provides low-overhead interprocess comms
  - How does this work? 
    - Crew has a controller & workers (which are R processes)
    - `mirai::daemons()`
    - 

#### Deploying data applications and documents to the cloud - Alex Chisholm


### Session 2 - R & Python - 1300hrs


#### Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail - Alenka Frim and Nic Crane

  - Everyone should be allowed to contribute in their own way. 
    - Contribute different skill sets
    - Choose right tools
  - Building diverse collaborations 
    - Dashboard content
    - actions
  - you'll go much further working together than trying to work alone

#### Python Rgonomics - Emily Riederer 

  - The "Rgonomic" stack: 
  - Data Exploration
    - Polars
      - Functional paradigm & expansive api
      - Highly performant
  - human-centered syntactic sugar
  - Visualizations: 
    - Plotnine: a high-fidelity clone of ggplot
    - seasborn: Similar philosophy of ggplot with its own pythonic syntax
  - Quarto balances interactivity and reproducibility and gets away from some of the BS with Jupyter notebooks
  - Great Tables package
    - isn't a direct clone of the GT package but rather trys to leverage some of the python powers while giving a similar experiences
  - So where to start? 
    - Dependency management woes



#### Quarto Templates

  - Why? 
    - save time with specific implementations
    - analytical skeleton
  - how? 
    - add custom css/scss files to the yaml
    - you can tinker and add it right to the `theme` and `css` calls
    - set up your text classes
  - (Chrome) DevTools: right-click -> inspect 
    - Then you can vie the skeleton of the rendered document. This will give you the div class and ids
    - Bottom right shows all the styles that will be moved over
    - USE THIS!!! When you come across other folks quarto docs, dive into it and inspect it. View whats going on. 
  - mdm web docs (mozilla web docs). 
    - has a ton of info on the css propoerties 
  - How to share the templates? 
    Do you have internal packages you can share? 
    - make a function: eg `create_cfx_html()` with the files to copy & the function to copy the files. 
      - Files to copy: 
        - inst/ directory
          - extdata/_extensions
            - cfx-html
              - styles.css
              - theme.scss
              - template.qmd
  - meghan.rbind.io
  - https://zelusanalytics.com/

### Data Visualizations: Idea > Process > Sharing track - 1440hrs


#### API-first package deisgn - Thomas Lin Pedersen

  - There's really no upper bound for your data viz creations
  - A good API design can help you achieve those upper bounds
  - Have a mission
    - Coherence doesn't just come organically
    - Think about the cohesion upfront, starting with it and how you want things to work together. 
    - missions can be contagious and go permeate through multiple projects
  - Maintenance is  a constant battle against API impurity
  - Simplicity
  - NOTE: Theres been a ton of awesome updates to the `patchwork` package. 
  - Embrace the "NO". 
  - 

  #### From idea to code to image: creative data viz in R

  - Why? 
    - Creative viz stands out
    - Emotional connection, engagement
    - Better and more clear comms
    - Self-expressions
  - Creativity
    - Novelty: come up with something new
    - Useful sols
    - Transform and synthesize
    - Connecting seemingly unrelated concepts
  - R & ggplot2
    - ggplot2 extensions
  - The process: 
    - Finding inspriation
      - Look at other peoples works
      - Dissect their choices 
      - Imitate style or techniques
      - Remix
      - Try out new packages
      - Look into charts typically used in other fields
      - Sketch what you want to do 
      - Try breaking the rules
      - Try data art  
    - Getting creative
      - look at other peoples code (eg #tidytuesday)
      - recreate data vizs made w/ other tools
      - Try out diff options in ggplot2 funcs
      - try to break things
      - Creative use of geoms
      - use text?? `geom_text()`
      - Create your own geoms
      - use `ggplot_build()`
      - Participating in challenges: experiment, learn, get feedback
    - Make it personal
    
#### Be Kind, Rewind - Ellis Hughes

### Keynote 2 - Data Wrangling like a boss w/ DuckDB - Hannes Muhler

  - Trad'l db sys's can be frustrating and slow
  - duckdb can quickly modify csv to parquet `copy xxx.csv to xxx.parquet`
  - Create duckdb files that stores the transactions and resulting files. 
  - `duckdb` file format is now considered stable
  - read remotely too
  - The views are stored in the file as well. 
  - now you can also create macros
    - `create macro not_total(col) as col not like "xxx%"`
  - 